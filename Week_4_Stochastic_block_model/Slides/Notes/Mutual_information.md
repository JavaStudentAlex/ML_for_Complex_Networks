#note #mutual_information #entropy #metric 

This metric shows how much information is shared between two random variables.

$$MI(X; Y) = \sum_{x, y} p(x, y) \log_2 \frac{p(x, y)}{p(x) p(y)}$$

It has a value from 0 to $\infty$.
