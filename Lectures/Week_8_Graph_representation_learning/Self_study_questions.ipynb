{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Under which condition does the covariance matrix of two random variables X, Y correspond to the dot product?\n",
    "The X and Y are centered, so their mean is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. What is a principal component and how can we compute it?\n",
    "It is such a rotation in the coordinate system that maximizes data variance. It is largest eigenvalue and correponding eigenvector of the covariance matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. What is the product of a data point $X_i \\in R^d$ with a principal component?\n",
    "It is an embedding of point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Explain the result of a multiplication of the data matrix $X \\in R^{d \\times n}$ with the eigenmatrix $Ð“ \\in R^{d \\times d}$ in PCA.\n",
    "It is just a rotation of the coordinate systemin the direction of the maximum variance, keep number of dimensions the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Which loss function is minimized by an embedding that is generated by the eigendecomposition of the adjacency matrix. What implication does this have for the similarity measure that we should use in the resulting feature space.\n",
    "\n",
    "$L = \\sum_{i, j} || \\vec{x}_i \\vec{x}_j - A_{ij}||^2$\n",
    "\n",
    "So we can approximate entries of the adjacency matrix by the dot product:\n",
    "\n",
    "$A_{ij} = \\vec{x}_i \\vec{x}_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. What is the Eckart-Young theorem and what does it tell us about the application of singular value decomposition to an adjacency matrix of a graph?\n",
    "It says that the truncated svd provides the optimal low-rank approximation in the termss of the Frobenius norm. So we can take an embedding of the lower dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Discuss whether you can apply Laplacian Eigenmaps to a network with multiple connected components. How do you have to adjust the embedding?\n",
    "\n",
    "Considering that fact that number of connected components means number of zeros in eigenvalues list o we will need to shift the embeddings into multiple positions to the right to avoid trivial solutions."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
