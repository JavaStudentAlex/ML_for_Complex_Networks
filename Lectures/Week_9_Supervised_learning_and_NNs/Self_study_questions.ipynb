{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Explain the difference between supervised link prediction and unsupervised similarity-based link prediction.\n",
    "In the unsupervised link prediction we used predefined train links set to generate a similarity matrix between all nodes. Yes we used links but in a latent way. Same happened in the test set when we generated negative samples and evaluated them with TopK method. So we use links between nodes but in a latent way. In the supervised way we define existing link as a class 1 and non-existing as class 2. So we use directly the connections.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Explain how the logit and the logistic function are used in logistic regression?\n",
    "\n",
    "Logit is a function which maps probabilities $logit(p) = \\log_b \\frac{p}{1-p}$. In the case $b = e$ the logit and logistic regresion are inversed. $\\sigma(logit(p)) = p$ and $logit(\\sigma(x)) = x$. So logitic regresion is used to generate probabilities. Logit`s is used to convert it back to linear combination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. How can we estimate the parameters of a logistic regression model?\n",
    "MLE or SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. What is the Hadamard operator and how we can use it for link prediction?\n",
    "Hadamard product is an element-wise product $x_{ij} = x_i x_j$ which i used to calculate features for node pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. What is the difference between the loss function in the perceptron classifier and the likelihood function in logistic regression?\n",
    "The perceptron loss function is MSE and the logistic regression used here is MLE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. How are the gradients of a perceptron affected by the choice of the loss/activation function?\n",
    "We are using chain rule so all gradients are affected by the loss function and activation functions derivatives?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Why is it convenient to include a factor $\\frac{1}{2}$ in the L2 loss function?\n",
    "Taking the derivative of the $a^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Explain the perceptron learning algorithm. How are the gradients of the loss function calculated?\n",
    "The output of the MLP is the superposition of activation functions and loss function. We apply chain rule to calculate the gradient of each posible weight. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Calculate the update rule of the perceptron learning algorithm for a step activation function that assigns class 1 for $f(\\vec{x})$ > 0 and class 0 else.\n",
    "\n",
    "We will need to take care about only the derivative of this function. But this function can not be differentiated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Explain how we can compute gradients of the loss function for feed-forward neural networks.\n",
    "still same chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. What is the difference between deterministic and stochastic gradient descent optimization?\n",
    "Deterministic GD is calculated in batches and stochatic with repect to each training example individually."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
