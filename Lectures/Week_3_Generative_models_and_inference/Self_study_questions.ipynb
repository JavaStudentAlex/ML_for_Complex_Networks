{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Write down an algorithmic formulation (in “pseudocode”) of the network generation process underlying the G(n, p) and the G(n, m) model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$G_{nm}$ model:\n",
    "```python\n",
    "def Gnm(n, m):\n",
    "\n",
    "    network: pp.Network = pp.Network(directed=False)\n",
    "    edges_added: int = 0\n",
    "\n",
    "    # Add nodes with zero-based numbers as UIDs\n",
    "    for i in range(n):\n",
    "        network.add_node(str(i))\n",
    "    \n",
    "    # Add m edges at random\n",
    "    while edges_added < m:\n",
    "\n",
    "        # Choose two nodes with replacement:\n",
    "        v, w = np.random.choice(list(network.nodes.uids), size=2, replace=True)\n",
    "\n",
    "        # avoid multi-edges\n",
    "        if (v,w) not in network.edges:\n",
    "            network.add_edge(v,w)\n",
    "            edges_added += 1\n",
    "    return network\n",
    "```\n",
    "\n",
    "$G_{np}$ model:\n",
    "```python\n",
    "def Gnp(n, p):\n",
    "\n",
    "    network: pp.Network = pp.Network(directed=False)\n",
    "\n",
    "    # add nodes with numeric uids\n",
    "    for i in range(n):\n",
    "        network.add_node(str(i))\n",
    "\n",
    "    # iterate through all node indices (incl. self-loops)\n",
    "    for s in range(n):\n",
    "        for t in range(s+1):          \n",
    "            # perform Bernoulli trial\n",
    "            if np.random.random() < p:\n",
    "                network.add_edge(str(s), str(t))\n",
    "    return network\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What is a statistical ensemble in the context of networks? What is a macrostate? What is a microstate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probabilistic generative model with parameters $\\Theta$ defines a statistical ensemble of networks, i.e. probability space with sample space $\\Omega(\\Theta)$ and probability measure $P$ where:\n",
    "1. $\\Omega(\\Theta)$ is the set of all graphs $G$ consistent with $\\Theta$\n",
    "2. $P(G|\\Theta) : G → [0, 1]$ for each $G \\in \\Theta(X)$\n",
    "\n",
    "$\\Theta$ defines the macrostate of the ensemble. Graphs $G ∈ \\Omega(\\Theta)$ are called microstates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. What is the microstate probability of the empty network in the G(n, m) model with n = 5 and m = 10?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Can you find a parameter of the G(n, p) model such that all microstates in the resulting statistical ensemble have the same probability?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$0.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. What is a probabilistic generative model and what is the likelihood function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probabilistic generative model is a model that describes how a graph is generated. We can use it to generate samples of graphs (so-called microstates). Each graph is generated with a microstate probability that is given by the model.\n",
    "\n",
    "Likelihood is a plausability of model with parameters $\\Theta$ given observation $X$.\n",
    "$$\n",
    "\\begin{align*}\n",
    "L(\\Theta | X) = P(X | \\Theta) = p^m (1 - p)^{\\binom{n}{2} - m}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Explain how we can use probabilistic generative models of graphs for statistical inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using a probabilistic generative model we can assume which distribution of microstates/degrees does this case follow and use it in statistical inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Explain the difference between the likelihood function in frequentist inference and the posterior probability in Bayesian inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likelihood estimates only one point and Bayesian statistics models the whole distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Why is the maximization of the likelihood function equivalent to the maximization of the log-likelihood function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log function is monotone."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
