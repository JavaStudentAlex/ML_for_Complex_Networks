{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. How is the entropy of a probability mass function defined?\n",
    "\n",
    "For the discrete random variable X:\n",
    "$$\n",
    "\\begin{align*}\n",
    "H(X) &= - \\sum_i P(X = i) \\log_2 P(x = i)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "For the continuous case:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "H(X) &= - \\int_i f(x) \\ln f(x) dx\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Calculate the entropy of the G(n, p) model and explain for which value p it is maximized.\n",
    "\n",
    "p = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Investigate the definition of the Kullback-Leibler divergence and explain its interpretation in terms of entropy.\n",
    "\n",
    "However, we can consider the entropy of a probability distribution, say $P(k)$, relative to another probability distribution $Q(k)$. The idea is that we want to capture the expected surprise about the outcome of random events with an actual probability distribution $P(k)$, when our model for the outcome of random events is given by $Q(k)$.\n",
    "\n",
    "The relative entropy (also called the Kullback-Leibler divergence) **from Q to P** for a discrete random variable with outcomes $i$ is given as: \n",
    "\n",
    "$$ D_{\\text{KL}}(P \\| Q) := - \\sum_{i} P(i) \\cdot \\log \\frac{Q(i)}{P(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Explain under which conditions the maximization of likelihood corresponds to a minimization of entropy.\n",
    "\n",
    "Since we have equiprobable microstates, the likelihood of our model parameters is simply the inverse of the number of realizations. The smaller that number, the larger the probabilities of equiprobable microstates (and thus the larger the likelihood) and the smaller the entropy. This means that in the micro-canonical ensemble, the parameters with maximum likelihood corresponds to those parameters for which the ensemble has minimal entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Explain how we can construct a Huffman code for a sequence of random variables.\n",
    "\n",
    "We would assign more frequent pairs to shorter codes.\n",
    "\n",
    "```python\n",
    "def huffman_tree(sequence):\n",
    "\n",
    "    counts = Counter(sequence).most_common()\n",
    "    seq_length = len(sequence)\n",
    "\n",
    "    # symbols with lowest frequency have highest priority\n",
    "    q = queue.PriorityQueue()\n",
    "    \n",
    "    labels = {}\n",
    "    node_type = {}\n",
    "    for (symbol, count) in counts:\n",
    "        # create leaf nodes and add to queue\n",
    "        labels[symbol]='{0} / {1:.3f}'.format(symbol, count/seq_length)\n",
    "        q.put((count, symbol))\n",
    "        node_type[symbol] = 'leaf'        \n",
    "    # Create huffman tree\n",
    "    i = 0\n",
    "    \n",
    "    edges = []\n",
    "    edge_symbols = []\n",
    "    \n",
    "    while q.qsize()>1:\n",
    "\n",
    "        # retrieve two symbols with minimal frequency\n",
    "        left = q.get()\n",
    "        right = q.get()\n",
    "\n",
    "        total_frequency = left[0] + right[0]\n",
    "\n",
    "        # create internal node v with total frequency as label\n",
    "        v = 'n_' + str(i)\n",
    "        label = '{:.2f}'.format(total_frequency/seq_length)\n",
    "        labels[v] = label\n",
    "        node_type[v] = 'internal'\n",
    "        edges.append((v,left[1]))\n",
    "        edge_symbols.append('0')\n",
    "        edges.append((v,right[1]))\n",
    "        edge_symbols.append('1')\n",
    "\n",
    "        q.put((left[0] + right[0], v))\n",
    "        i += 1\n",
    "\n",
    "    # the remaining entry corresponds to the root node\n",
    "    root = q.get()\n",
    "    huffman_tree=pp.Graph.from_edge_list(edges)\n",
    "    huffman_tree.data.node_labels = [labels[v] for v in huffman_tree.nodes]\n",
    "    huffman_tree.data.node_type = [node_type[v] for v in huffman_tree.nodes]\n",
    "    huffman_tree.data.edge_symbols = edge_symbols\n",
    "    return huffman_tree, root[1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. What statement does Shannon’s source coding theorem make?\n",
    "\n",
    "Let $H_i$ be a sequence of n i.i.d. random variables with entropy $H(X_i)$. For any $\\epsilon$ > 0 and sufficiently large n there exists a coding scheme that encodes a sequence of n realizations $X_i$ in terms of $n * H(X_i)$ bits such that the sequence can be recovered with probability larger than $1 - \\epsilon$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Use Shannon’s source coding theorem to calculate the optimal compression for a sequence of biased coin tosses with $p \\neq 0.5$.\n",
    "\n",
    "Example at slide 14."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.How can entropy be used for community detection based on the stochastic block model?\n",
    "\n",
    "Entropy minimization is equivalent to likelihood maximization in the $G(n,p)$ version of the block model search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. How is the description length of the stochastic block model defined?\n",
    "\n",
    "We will first define a function $h(x) = (1 + x) \\log(1 + x) - x \\log x$. Based on it we will describe a description length term:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\triangle{\\vec{z}} &= m h \\left( \\frac{B(B+1)}{2m} \\right) + n \\log B\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Explain how the detection of the optimal number of communities in a network is related to Occam’s razor.\n",
    "\n",
    "We try to find with maximum explaratory power and minimum system difficulties."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
