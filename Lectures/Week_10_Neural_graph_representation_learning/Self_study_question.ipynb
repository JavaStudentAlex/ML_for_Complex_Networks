{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What is the distributional hypothesis in linguistics and how we can use to embed words in a vector space?\n",
    "\n",
    "We can assess the meaning of the word by the context word happening in the neirest neighbourhood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Explain how we can use neural networks to find approximate factorizations of matrices.\n",
    "\n",
    "Matrix factorization is literally splitting a matrix into multiplication of other lower dimensions matrices. Considering that fact that NN layers are literally matrix multiplication the whole forward pass is just a matrix factorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Explain whether the power law distribution of degrees is likely to affect node embeddings generated by DeepWalk.\n",
    "\n",
    "Considering that fact that power law generates heterogenious network with hubs as centers, it means that a walker would stuck in the only one group. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Investigate the concept of negative sampling in the context of word2vec and explain how it is used to speed up the training process.\n",
    "\n",
    "We sample negative and positive examples and update not the whole dictionary, but jut sampled nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Consider the activation and loss function used in the SkipGram model of word2vec. Explain how the SkipGram model is related to maximum likelihood estimation of a multinomial logistic regression model.\n",
    "\n",
    "Activation - softmax, loss - crosss entropy. The SkipGram data itself uses the MLE to find the best fitting parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. What is the difference between DeepWalk and node2vec? Explain the influence of the parameters p and q in node2vec.\n",
    "\n",
    "DeepWalk is a random walker with uniform chance to choose the next nodem, and node2vec model is biased towards some style of behaving. Parameter p - revisit node we came from, q - chance to isit nodes which are the neighbours of the node where we recently came from. Based on the q we define if we do a BFS/DFS walk. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. What is alias sampling and how can we use it to efficiently sample large numbers of biased random walks.\n",
    "\n",
    "Every time we sample a node the probability distribution changes abruply and it takes some time to init a sampler. And it becomes a bottleneck. That is why we are using alias sampling which is fast in this tas. It uses two arrays: probabilities and aliases which allows to sample a random variable fast and with predefined probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. How are DeepWalk and node2vec related to the representation learning techniques introduced in Lecture 08.\n",
    "\n",
    "We could see that all methods defined in the lecture 8 which generated embeddings directly used matrix factorization as a main principle. Same happends here as DeepWalk and node2vec use SkipGram model to generate embeddings and thi method is literally a matrix factorization. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
